{
  "hash": "88227d4e3ee28ba1a2816fa45cc03865",
  "result": {
    "markdown": "# The Subjective Worlds of Frequentist and Bayesian Statistics {#sec-subjective-worlds}\n\n## File setup {.unnumbered}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glossary)\nglossary_path(\"glossary.yml\")\nglossary_popup(\"hover\")\n```\n:::\n\n\n```{.r .cell-code}\nglossary_style(color = \"#0066cc\", \n               text_decoration = \"underline double 1px\",\n               def_bg = \"#333\",\n               def_color = \"white\")\n```\n\n<style>\na.glossary {\n  color: #0066cc;\n  text-decoration: underline double 1px;\n  cursor: help;\n  position: relative;\n}\n\n/* only needed for popup = \"click\" */\n/* popup-definition */\na.glossary .def {\n  display: none;\n  position: absolute;\n  z-index: 1;\n  width: 200px;\n  bottom: 100%;\n  left: 50%;\n  margin-left: -100px;\n  background-color: #333;\n  color: white;\n  padding: 5px;\n  border-radius: 6px;\n}\n/* show on click */\na.glossary:active .def {\n  display: inline-block;\n}\n/* triangle arrow */\na.glossary:active .def::after {\n  content: ' ';\n  position: absolute;\n  top: 100%;\n  left: 50%;\n  margin-left: -5px;\n  border-width: 5px;\n  border-style: solid;\n  border-color: #333 transparent transparent transparent;\n}\n</style>\n\n\n## Mission Statement\n\nMission is to understand the purpose of statistical inference.\n\n## Chapter Goals\n\nBayesian statistics allows us to go from what is known -- the *data*\n(e.g., the results of the coin throw) -- and extrapolate backwards to\nmake probabilistic statements about the parameters (the underlying bias\nof the coin) of the processes that were responsible for its generation.\nIn Bayesian statistics, this inversion process is carried out by\napplication of <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (SR2, Chap.2)'>Bayes’ rule</a>\n\n## Bayes' Rule -- Allowing us to go From the Effect Back to its Cause\n\n::: {#exm-example-crooked-casino}\n#### Crooked Casino\n\nSuppose that we know that a casino is crooked and uses a loaded die with\na probability of rolling a 1, that is\n$\\frac{1}{3} = 2 \\times \\frac{1}{6}$, twice its unbiased value.\n\n$$\nPr(1,1 | \\text{crooked casino}) = \\frac{1}{3} \\times \\frac{1}{3} = \\frac{1}{9}\n$$ {#eq-crooked-casino}\n\n-   **Pr**: Here we use `Pr` to denote a probability, with the comma\n    here having the literal interpretation of *and.* Hence, `Pr(1, 1)`\n    is the probability we obtain a 1 on the first roll *and* a 1 on the\n    second.\n-   **\\|**: The vertical line, `|`, here means *given* in probability,\n    so `Pr(1,1|crooked casino)` is the probability of throwing two\n    consecutive 1s *given that* the casino is crooked.\n:::\n\nWith Bayes' rule you can go from\n\n$$\nPr(effect|case) → Pr(cause|effect)\n$$ {#eq-opposite-direction} In order to take this leap, you need\n\n***\n::: {#thm-bayes-rule}\n#### Bayes' theorem\n\n$$\nPr(effect|cause) = \\frac{Pr(cause|effect) \\times Pr(cause)}{Pr(effect)}\n$$ {#eq-bayes-theorem}\n:::\n***\n\nContinuing our @exm-example-crooked-casino:\n$Pr(\\text{crooked casino}|1,1)$ is the probability that the casino is\ncrooked given that we rolled two 1s. **This process where we go from an\neffect back to a cause is the essence of inference.**\n\n## The Purpose of Statistical Inference\n\nThere are two predominant schools of thought for carrying out this\nprocess of inference: Frequentist and Bayesian.\n\n## The World According to Frequentists\n\nIn Frequentist (or Classical) statistics, we suppose that our sample of\ndata is the result of one of an infinite number of exactly repeated\nexperiments. The sample of coin flips we obtain for a fixed and finite\nnumber of throws is generated as if it were part of a longer (that is,\ninfinite) series of repeated coin flips. In Frequentist statistics the\ndata are assumed to be *random* and results from *sampling* from a fixed\nand defined *population* distribution. For a Frequentist the noise that\nobscures the true signal of the real population process is attributable\nto *sampling variation* -- the fact that each sample we pick is slightly\ndifferent and not exactly representative of the population.\n\n## The World According to Bayesians\n\nBayesians do not imagine repetitions of an experiment in order to define\nand specify a probability. Bayesians do not view probabilities as\nunderlying laws of cause and effect. They are merely abstractions which\nwe use to help express our uncertainty. In this frame of reference, it\nis unnecessary for events to be repeatable in order to define a\nprobability. For Bayesians, probabilities are seen as an expression of\nsubjective beliefs, meaning that they can be updated in light of new\ndata. Bayesians assume that, since we are witness to the data, it is\nfixed, and therefore does not vary. We do not need to imagine that there\nare an infinite number of possible samples, or that our data are the\nundetermined outcome of some random process of sampling. We never\nperfectly know the value of an unknown parameter.\n\n## Do Parameters Actually Exist and have a Point Value?\n\nFor Bayesians, the parameters of the system are taken to vary, whereas\nthe known part of the system -- the data -- is taken as given.\nFrequentist statisticians, on the other hand, view the unseen part of\nthe system -- the parameters of the probability model -- as being fixed\nand the known parts of the system -- the data -- as varying.\n\nIn the Bayesian approach, parameters can be viewed from two\nperspectives. Either we view the parameters as truly *varying*, or we\nview our knowledge about the parameters as imperfect. The fact that we\nobtain different estimates of parameters from different studies can be\ntaken to reflect either of these two views.\n\nThe Frequentist view of parameters as a limiting value of an average\nacross an infinity of identically repeated experiments runs into\ndifficulty when we think about one-off events. For example, the\nprobability that the Democrat candidate wins in the 2020 US election\ncannot be justified in this way, since elections are never rerun under\nthe exact same conditions.\n\n## Frequentist and Bayesian Inference\n\nThe Bayesian inference process is the only logical and consistent way to\nmodify our beliefs to account for new data. Before we collect data we\nhave a probabilistic description of our beliefs, which we call a\n*prior.* We then collect data, and together with a model describing our\ntheory, Bayes' formula allows us to calculate our post-data or\n*posterior* belief:\n\n$$\nprior + data → (model) → posterior\n$$ {#eq-from-prior-to-posterior}\n\nIn inference, we want to draw conclusions based purely on the rules of\nprobability. If we wish to summarise our evidence for a particular\nhypothesis, we describe this using the language of probability, as the\n'probability of the hypothesis given the data obtained'. The difficulty\nis that when we choose a probability model to describe a situation, it\nenables us to calculate the 'probability of obtaining our data given our\nhypothesis being true' -- the opposite of what we want. This probability\nis calculated by accounting for all the possible samples that could have\nbeen obtained from the population, if the hypothesis were true. The\nissue of statistical inference, common to both Frequentists and\nBayesians, is how to invert this probability to get the desired result.\n\nFrequentists stop here, using this inverse probability as evidence for a\ngiven hypothesis. They assume a hypothesis is true and on this basis\ncalculate the probability of obtaining the observed data sample. If this\nprobability is small, then it is assumed that it is unlikely that the\nhypothesis is true, and we reject it. In our coin example, if we throw\nthe coin 10 times and it always lands heads up (our data), the\nprobability of this data occurring given that the coin is fair (our\nhypothesis) is small. In this case, Frequentists would reject the\nhypothesis that the coin is fair. Essentially, this amounts to setting\n$Pr(hypothesis|data)=0$. However, if this probability is not below some\narbitrary threshold, then we do not reject the hypothesis. But\nFrequentist inference is then unclear about what probability we should\nascribe to the hypothesis. Surely it is non-zero, but exactly how\nconfident are we in it? In Frequentist inference we do not get an\naccumulation of evidence for a particular hypothesis, unlike in Bayesian\nstatistics.\n\nIn Bayesian inference, there is no need for an arbitrary threshold in\nthe probability in order to validate the hypothesis. All information is\nsummarised in this (posterior) probability and there is no need for\nexplicit hypothesis testing. However, to use Bayes' rule for inference,\nwe must supply a prior -- an additional element compared to Frequentist\nstatistics. The prior is a probability distribution that describes our\nbeliefs in a hypothesis before we collect and analyse the data. In\nBayesian inference, we then update this belief to produce something\nknown as a posterior, which represents our post-analysis belief in the\nhypothesis.\n\n### The Frequentist and Bayesian Murder Trials\n\nIf you choose the Frequentist trial, your jurors start by specifying a\nmodel based on previous trials, which assigns a probability of your\nbeing seen by the security camera if you were guilty. They use this to\nmake the statement that 'If you did commit the murder, then 30% of the\ntime you would have been seen by the security camera' based on a\nhypothetical infinity of repetitions of the same conditions. Since\n$Pr(\\text{you were seen by the camera}|guilt)$ is not sufficiently\nunlikely (the p value is not below 5%), the jurors cannot reject the\nnull hypothesis of guilt, and you are sentenced to life in prison.\n\nIn the Bayesian trial there is collected different kinds of evidence\nagainst that you are guilty: In addition to the 30% of camera footage\nthere is an array of evidence, which suggests - that you neither knew\nSally - nor had any previous record of violent conduct, being otherwise\na perfectly respectable citizen. - Furthermore, Sally's ex-boyfriend is\na multiple offending-violent convict on the run from prison after being\nsentenced by a judge on the basis of Sally's own witness testimony.\n\nUsing this information, the jury sets a prior probability of the\nhypothesis that you are guilty equal to $\\frac{1}{1000}$. Using\n<a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (SR2, Chap.2)'>Bayes’ rule</a> the jury concludes that\nthe probability of your committing the crime is only $\\frac{1}{1000}$.\n\n------------------------------------------------------------------------\n\n::: {#exm-murder-trial}\n#### Bayesian Murder Trial\n\n$$\n\\begin{align*}\np(guilt | \\text{security camera footage}) = \\frac{p(\\text{security camera footage})\\times p(gilt)}{p(\\text{security camera footage})}\\\\\n= \\frac{\\frac{30}{100} \\times \\frac{1}{1000}}{\\frac{30}{100} \\times \\frac{999}{1000} + \\frac{30}{100} \\times \\frac{1}{1000}} \\\\\n= \\frac{1}{1000}\n\\end{align*}\n$$ {#eq-murder-trial}\n:::\n\n------------------------------------------------------------------------\n\n::: callout-note\n#### Note: How to calculate the denominator?\n\nI do understand the formula but have difficulty with calculation of the\ndenominator: My personal explication is a kind of normalization using\nthe the guilt and not guilt probabilities.\n:::\n\n### Radio control towers\n\nThe essence in my understanding of this example is that in the Bayesian\napproach there is a probability distribution and not just a point\nestimate as in the Frequentist point of view.\n\n## Bayesian Inference Via Bayes' Rule\n\nThe Bayesian inference process uses\n<a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (SR2, Chap.2)'>Bayes’ rule</a> to estimate a probability\ndistribution for those unknown parameters after we observe the data.\n\nThe term \"probability distribution\" will be explained in @sec-probability.\n\n***\n::: {#thm-bayes-inference}\n#### Bayes’ rule used in statistical inference\n\n$$\nPr(𝛩|data) = \\frac{Pr(data|𝛩) \\times Pr(𝛩)}{Pr(data)}\n$$ {#eq-bayes-inference}\n\n-   $p$ indicates a probability distribution which may represent either\n    probabilities or, more usually, probability densities.\n-   $𝛩$ represents the unknown parameter(s) which we want to estimate.\n\n:::\n***\n\n### Likelihood\n\n`Pr(data|θ)` as the firt term in the numerator of @eq-bayes-inference is called the *likelihood*, which is common to both Frequentist and Bayesian analyses. It tells us the probability of generating the particular sample of data if the parameters in our statistical model were equal to $θ$. When we choose a statistical model, we can usually calculate the probability of particular outcomes, so this is easily obtained.\n\nMore about the understanding of \"likelihoods\" in @sec-likelihoods.\n\n### Priors\n\n`p(θ)` as the second term in the numerator of @eq-bayes-inference is the most controversial part of the Bayesian formula, is called the prior distribution. It is a probability distribution which represents our pre-data beliefs across different values of the parameters in our model, $θ$.\n\nThe concept of priors is covered in detail in @sec-priors.\n\n\n### The Denominator\n\n`p(data)` in the denominator of @eq-bayes-inference represents the probability of obtaining our particular sample of data if we assume a particular model and prior. \n\nA detailed discussion is postponed until @sec-devil-denominator.\n\n### Posteriors: the goal of Bayesian inference\n\n`p(θ|data)` is the posterior probability distribution and the main goal of Bayesian inference. The posterior distribution summarises our uncertainty over the value of a parameter. If the distribution is narrower, then this indicates that we have greater confidence in our estimates of the parameter’s value. The posterior distribution is also used to predict future outcomes of an experiment and for model testing.\n\nMore details in @sec-posteriors.\n\n## Implicit Versus Explicit Subjectivity\n\nOne of the major arguments levied against Bayesian statistics is that it is subjective due to its dependence on the analyst specifying their pre-experimental beliefs through priors. \n\n1. *All* analyses involve a degree of subjectivity, which is either explicitly stated or, more often, implicitly assumed.\n2. In a Frequentist analysis, the statistician typically selects a model of probability which depends on a range of assumptions. For example, the simple *linear regression model* is often used, without justification, in applied Frequentist analyses.\n3. In applied Frequentist's research, there is a tendency among scientists to choose data to include in an analysis to suit one’s needs, although this practice should really be discouraged.\n4. A further source of subjectivity is the way in which models are checked and tested.\n\nIn contrast to the examples of subjectivity mentioned above, Bayesian priors are explicitly stated. Furthermore, the more data that is collected, (in general) the less impact the prior exerts on posterior distributions. In any case, if a slight modification of priors results in a different conclusion being reached, it must be reported by the researcher.\n\nThe choice of a threshold probability of 5% or 1% – known as a statistical test’s size – is completely arbitrary, and subjective. In Bayesian statistics, we instead use a subjective prior to invert the likelihood from $p(data|θ) → p(θ |data)$. There is no need to accept or reject a null hypothesis and consider an alternative since all the information is neatly summarised in the posterior. In this way we see a symmetry in the choice of Frequentist test size and Bayesian priors; they are both required to invert the likelihood to obtain a posterior.\n\n## Chapter Summary\n\nThis chapter has focused on the philosophy of statistical inference. Statistical inference is the process of inversion required to go from an effect (the data) back to a cause (the process or parameters). The trouble with this inversion is that it is generally much easier to do things the other way round: to go from a cause to an effect. Frequentists and Bayesians start by defining a forward probability model that can generate data (the effect) from a given set of parameters (the cause). The method that they each use to run this model in reverse and determine the probability for a cause is different. Frequentists and Bayesians also differ in their view on probabilities.\n\n## Chapter Outcomes\n\nThe reader should now be familiar with the following concepts:\n\n- the goals of statistical inference\n- the difference in interpretation of probabilities for Frequentists versus Bayesians\n- the differences in the Frequentist and Bayesian approaches to inference\n\n## Appendix (empty)\n\n## Problem Sets\n\n### Problem 2.1: The deterministic nature of random coin throwing\n\n## I STOPPED HERE (2023-09-01)\n",
    "supporting": [
      "02-subjective-worlds_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}