{
  "hash": "686a6a9b97037af3f76c1f3973819bbd",
  "result": {
    "markdown": "# 2D continuous probability example {#sec-annex-example-1}\n\n\n::: {.cell}\n\n:::\n\n\n\n::: callout-note\nIn this section I try to reproduce Figure 3.7. (Lambert, Ben. A\nStudent's Guide to Bayesian Statistics (S.44). SAGE Publications.\nKindle-Version.)\n\n![Foot length and literacy: a two-dimensional continuous probability\nexample](img/fig-3-7.jpeg){#fig-3-7}\n:::\n\nTo reproduce Figure 3.7 I would need a data set. As I couldn't find the\ndata Lambert is referring to (\"Foot length and literacy\") I am going to\nuse another data set about the relation between shoe size and height\n[@mclaren2012].\n\n::: {.callout-warning}\nThe links to the additional material in the article are not valid anymore. The correct URLs are:\n\n- **data set**: [https://jse.amstat.org/v20n3/mclaren/shoesize.xls](https://jse.amstat.org/v20n3/mclaren/shoesize.xls). There are also other places where you can inspect and download the data set, e.g. at the [Emerging Technology Institute](https://github.com/emtechinstitute/MachineLearning/blob/main/shoesize.csv).\n- **data documentation**: [https://jse.amstat.org/v20n3/mclaren/documentation.doc](https://jse.amstat.org/v20n3/mclaren/documentation.doc)\n- **instructor manual**: [https://jse.amstat.org/v20n3/mclaren/manual.doc](https://jse.amstat.org/v20n3/mclaren/manual.doc)\n:::\n\n## Figure 3.7: Left panel\n\nTo generate the left panel I have to look into packages for 3D graphic.\nI am at the moment not interested in the 3D display but I have googled\nsome interesting packages for later to learn and to apply:\n\n-   [graphics::persp()](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/persp.html%3E):\n    This Base R function draws perspective plots of a surface over the\n    x--y plane.\n-   [rgl::plot3d()](https://r-graph-gallery.com/3d.html#3dscatter):\n    Draws a 3D interactive scatterplot but also other 3D visualizations\n    by using [OpenGL](https://www.opengl.org/), the industry standard\n    for high performance graphics. (I think \"rgl\" stands for \"R Graphic\n    Library\"). RGL is a 3D real-time rendering system for R.\n-   [plotly::plot_ly()](https://r-graph-gallery.com/3d-surface-plot.html):\n    With the argument `type=surface` {**plotly**} generates an\n    interactive surface plot. But {**plotly**} has many more option for\n    [plotting 3D graphs](https://plotly.com/r/3d-charts/).\n-   [scatterplot3d::scatterplot3d()](http://sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization#basic-3d-scatter-plots):\n    Plots a three dimensional (3D) point cloud.\n-   [plot3D::scatter3D()](http://www.sthda.com/english/wiki/impressive-package-for-3d-and-4d-graph-r-software-and-data-visualization):\n    It containing many functions for 2D and 3D plotting: `scatter3D()`,\n    `points3D()`, `lines3D()`, `text3D()`, `ribbon3D()`, `hist3D()`,\n    etc. (I could neither find the vignettes nor the github repo.)\n-   [car::scatter3d()](http://www.sthda.com/english/wiki/amazing-interactive-3d-scatter-plots-r-software-and-data-visualization):\n    The function `scatter3d()` uses the {**rgl**} package to draw and\n    animate 3D scatter plots.\n\n## Figure 3.7: Right panel\n\nThe <a class='glossary' title='Contour plots are a way to show a three-dimensional surface on a two-dimensional plane. A contour plot is appropriate if you want to see how some value Z changes as a function of two inputs, X and Y: z = f(x, y). Contour lines indicate levels that are the same. (Statistics How To)'>contour plot</a> of the right panel I could draw with\n`ggplot2::geom_contour()`. But I have still to learn how to calculate\nthe <a class='glossary' title='The two defining features are: (1) All values of the distribution must be real and non-negative. (2) The sum (for discrete random variables) or integral (for continuous random variables) across all possible values of the random variable must be 1. (Bayesian Statistics, Chap.3)'>probability distribution</a>.\n\n## Download data\n\nThe next code chunk has to run only once.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://raw.githubusercontent.com/emtechinstitute/MachineLearning/main/shoesize.csv\"\ntib <- readr::read_csv(url, col_types = \"ifdd\")\n\n## data cleaning: \n## only two values of the Height column are doubles. \n## Index 178 and 208 (row 179 & 209)\n## See details in \"/my_notes/changing-cell-values.qmd\"\n\ntib <- tib |> \n    dplyr::mutate(Height = round(Height, 0)) |> \n    dplyr::mutate(Height = vctrs::vec_cast(Height, integer()))\n\nsaveRDS(tib, \"data/shoesize.rds\")\n```\n:::\n\n\n## Scatterplot\n\nNow that I have the data I can inspect the data and plot the relation of shoe size and height. I am going to use the `position = \"jitter\"` argument to prevent overplotting and to show all data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntib <-  readRDS(\"data/shoesize.rds\")\nstr(tib)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Classes 'tbl_df', 'tbl' and 'data.frame':\t408 obs. of  4 variables:\n#>  $ Index : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ Gender: Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ Size  : num  5.5 6 7 8 8 9 7.5 6.5 5 6 ...\n#>  $ Height: int  60 60 60 60 60 60 60 60 60 61 ...\n```\n:::\n\n```{.r .cell-code}\ntib |> \n    ggplot2::ggplot(ggplot2::aes(Size, Height)) +\n    ggplot2::geom_point(position = \"jitter\")\n```\n\n::: {.cell-output-display}\n![](90-example-1_files/figure-html/scatterplot-data-1.png){width=672}\n:::\n:::\n\n\nAs I do not have experience with inches here are two websites for converting the values:\n\n- [Length conversion](https://www.unitconverters.net/length-converter.html)\n- [Shoe size conversion](https://www.zappos.com/c/shoe-size-conversion)\n\n::: {.callout-warning}\nThe same numbers for women and men shoes result in different length values!\n:::\n\n\n## Height distribution\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-dist-heights lst-cap=\"Plot the distribution of the heights, overlaid by an ideal Gaussian distribution\"}\ntib |> \n    ggplot2::ggplot(ggplot2::aes(Height)) +\n    ggplot2::geom_density() +  \n    \n    ggplot2::stat_function(\n        fun = dnorm,\n        args = with(tib, c(mean = mean(Height), sd = sd(Height)))\n    ) +\n    ggplot2::scale_x_continuous(\"Height in inches\") +\n    ggplot2::scale_y_continuous(\"Density\")\n```\n\n::: {.cell-output-display}\n![The distribution of the heights data, overlaid by an ideal Gaussian distribution](90-example-1_files/figure-html/fig-dist-heights-1.png){#fig-dist-heights width=672}\n:::\n:::\n\n\n## Defining priors for Gaussian heights model\n\n### Formula\n\n::: {#def-gaussian-heights-model}\n##### Define priors for $\\mu$ and $\\sigma$ for the Gaussian height model\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ)  \\\\ \n\\mu \\sim \\operatorname{Normal}(70, 8)  \\\\ \n\\sigma \\sim \\operatorname{Uniform}(0, 20)       \n\\end{align*}\n$$ {#eq-gaussian-heights-model}\n:::\n\n### Plot $\\mu$\n\nThe prior for $\\mu$ is a broad Gaussian prior, centered on 70 in (177.8 cm), with 95% of probability between 70 ± 16 in. The range from 54 in to 86 in (137.16 to 218,44cm) encompasses a huge range of plausible mean heights for human populations.\n\nIt is always important to plot the priors, so you get a sense of the assumption they build into the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-mean-prior lst-cap=\"Plot the chosen mean prior\"}\np1 <-\n  tibble::tibble(x = seq(from = 40, to = 100, by = .1)) |> \n    \n  ggplot2::ggplot(ggplot2::aes(x = x, y = dnorm(x, mean = 70, sd = 8))) +\n  ggplot2::geom_line() +\n  ggplot2::scale_x_continuous(breaks = seq(from = 40, to = 100, by = 10)) +\n  ggplot2::labs(title = \"mu ~ dnorm(70, 8)\",\n        x = \"height in inches\",\n       y = \"density\")\n\np1\n```\n\n::: {.cell-output-display}\n![Chosen mean prior](90-example-1_files/figure-html/fig-mean-prior-1.png){#fig-mean-prior width=672}\n:::\n:::\n\n\nThe chosen prior is assuming that the average height (not each individual height) is almost certainly between 43 and 97 in (=109,22 and 246,38 cm). So this prior carries a little information, but not a lot.\n\n### Plot $\\sigma$\n\nThe $\\sigma$ prior is a truly flat prior, a uniform one, that functions just to constrain σ to have positive probability between zero and 20 in (= 50.8 cm).\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-sigma-prior lst-cap=\"Plot the chosen sigma prior\"}\np2 <-\n  tibble::tibble(x = seq(from = -10, to = 30, by = .1)) |>  \n    \n  ggplot2::ggplot(ggplot2::aes(x = x, y = dunif(x, min = 0, max = 20))) +\n  ggplot2::geom_line() +\n  ggplot2::scale_x_continuous(breaks = seq(from = -10, to = 30, by = 10)) +\n  ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  ggplot2::ggtitle(\"sigma ~ dunif(0, 20)\")\n\np2\n```\n\n::: {.cell-output-display}\n![Chosen sd prior](90-example-1_files/figure-html/fig-sigma-prior-1.png){#fig-sigma-prior width=672}\n:::\n:::\n\n\nA standard deviation like $\\sigma$ must be positive, so bounding it at zero makes sense. How should we pick the upper bound? In this case, a standard deviation of 20 in (50.8 cm) would imply that 95% of individual heights lie within 40 in (101.6 cm) of the average height. That’s a very large range.\n\n### Prior predictive simulation\n\nThinking about the mean height value is one thing, but it is almost more important to see what these priors imply about the distribution of individual heights. This so-called <a class='glossary' title='It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4)'>prior predictive simulation</a> is an essential part of modeling. Once we’ve chosen priors for $h$, $\\mu$, and $\\sigma$, these imply a joint prior distribution of individual heights. By simulating from this distribution, we can see what our choices imply about observable height. This helps us diagnose bad choices. Lots of conventional choices are indeed bad ones, and we’ll be able to see this through prior predictive simulations.\n\nOkay, so how to do this? You can quickly simulate heights by sampling from the prior. Remember, every posterior is also potentially a prior for a subsequent analysis, so you can process priors just like posteriors. We can simulate from both priors at once to get a prior probability distribution of `Height`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-prior-predictive-sim lst-cap=\"Simulate heights by sampling from the priors:\"}\nn <- 1e4\nset.seed(4)\n\nsim <-\n  tibble::tibble(sample_mu    = rnorm(n, mean = 70, sd  = 8),\n         sample_sigma = runif(n, min = 0, max = 20)) |> \n  dplyr::mutate(height = rnorm(n, mean = sample_mu, sd = sample_sigma))\n  \np3 <- sim |> \n  ggplot2::ggplot(ggplot2::aes(x = height)) +\n  ggplot2::geom_density() +\n  ggplot2::scale_x_continuous(breaks = seq(from = 0, to = 140, by = 20)) +\n  ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  ggplot2::ggtitle(\"height ~ dnorm(mu, sigma)\") +\n  ggplot2::theme(panel.grid = ggplot2::element_blank())\n\np3\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the prior: tidyverse version](90-example-1_files/figure-html/fig-prior-predictive-sim-1.png){#fig-prior-predictive-sim width=672}\n:::\n:::\n\n\n\n## Linear model\n\nThe strategy is to make the parameter for the mean of a Gaussian distribution, μ, into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the <a class='glossary' title='A linear model specifies a linear relationship between a dependent variable and \\(n\\) independent variables. It conforms to a mathematical model represented by a linear equation of the form \\(Y = b_{1}X_{1} + b_{2}X_{2} + … + b_{n}X_{n}\\). (Oxford Reference)'>linear model</a>. The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship.\n\nHow do we get shoe size into the Gaussian model of height as specified in @eq-gaussian-heights-model? Let $x$ be the name for the column of shoe size measurements, `tib$Size`. Let the average of the $x$ values be $\\overline{x}$, “ex bar”. Now we have a predictor variable $x$, which is a list of measures of the same length as $h$. To get `Size` into the model, we define the mean $\\mu$ as a function of the values in $x$. This is what it looks like, with explanation to follow:\n\n::: {#def-height-size-linear-model}\n##### Linear model `Height` against `Size`\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta(x_{i}-\\overline{x}) \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(70, 8) \\space \\space (3)  \\\\ \n\\beta \\sim \\operatorname{Log-Normal}(0,1) \\space \\space (4) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 20) \\space \\space (5)      \n\\end{align*}\n$$ {#eq-height-size-linear-model}\n\n(1) **Likelihood (Probability of the data)**: The first line is nearly\n    identical to before, except now there is a little index $i$ on the\n    $μ$ as well as the $h$. You can read $h_{i}$ as \"each height\" and\n    $\\mu_{i}$ as \"each $μ$\" The mean $μ$ now depends upon unique values\n    on each row $i$. So the little $i$ on $\\mu_{i}$ indicates that *the\n    mean depends upon the row*.\n\n(2) **Linear model**: The mean $μ$ is no longer a parameter to be\n    estimated. Rather, as seen in the second line of the model, $\\mu{i}$\n    is constructed from other parameters, $\\alpha$ and $\\beta$, and the\n    observed variable $x$. This line is not a stochastic relationship\n    -- there is no `~` in it, but rather an `=` in it -- because\n    the definition of $\\mu{i}$ is deterministic. That is to say that,\n    once we know $\\alpha$ and $\\beta$ and $x_{i}$, we know $\\mu{i}$ with\n    certainty. -- At first we tried a prior with $\\beta \\sim \\operatorname{Normal}(0,10)$ but it turned out that this prior results in unrealistic data with negative heights. The Log-Normal ensures only positive values. See @fig-priors.\n\n(3) **includes (3),(4) and(5) with** $\\alpha, \\beta, \\sigma$ priors: The\n    remaining lines in the model define distributions for the unobserved\n    variables. These variables are commonly known as parameters, and\n    their distributions as priors. There are three parameters:\n    $\\alpha, \\beta, \\sigma$. You've seen priors for $\\alpha$ and $\\sigma$\n    before, although $\\sigma$ was called $\\mu$ back then.\n:::\n\n------------------------------------------------------------------------\n\n\n### Prior precitive simulation\n\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-priors lst-cap=\"Calcukate prior with Normal and Normal-Log\"}\n## start condition \nset.seed(2971)\n# how many lines to draw?\nn_lines <- 100\n\n## normal model \nlines <-\n  tibble::tibble(n = 1:n_lines,\n         a = rnorm(n_lines, mean = 70, sd = 8),\n         b = rnorm(n_lines, mean = 0, sd = 10)) |> \n  tidyr::expand_grid(size = range(tib$Size)) |> \n  dplyr::mutate(height = a + b * (size - mean(tib$Size)))\n\np4 <- \n  lines |> \n  ggplot2::ggplot(ggplot2::aes(x = size, y = height, group = n)) +\n  ggplot2::geom_line(alpha = 1/10) +\n  ggplot2::ggtitle(\"b ~ dnorm(0, 10)\") +\n  ggplot2::theme_classic()\n\n## log-normal model \n# make a tibble to annotate the plot\n\np5 <- \n    tibble::tibble(n = 1:n_lines,\n           a = rnorm(n_lines, mean = 70, sd = 8),\n           b = rlnorm(n_lines, mean = 0, sd = 1)) |> \n      tidyr::expand_grid(size = range(tib$Size)) |> \n      dplyr::mutate(height = a + b * (size - mean(tib$Size)))  |> \n  \n  # plot\n  ggplot2::ggplot(ggplot2::aes(x = size, y = height, group = n)) +\n  ggplot2::geom_line(alpha = 1/10) +\n  ggplot2::ggtitle(\"log(b) ~ dnorm(0, 1)\") +\n  ggplot2::theme_classic()\n\n## display plots\nlibrary(patchwork)\np4 + p5\n```\n\n::: {.cell-output-display}\n![The effects of two different beta priors](90-example-1_files/figure-html/fig-priors-1.png){#fig-priors width=672}\n:::\n:::\n\n\n### Posterior distribution\n\n#### Fit model with rehtinking priors\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-lm-rethinking-priors lst-cap=\"Fit model with priors from the rethinking book, display summary, trace plots and linear correlation\"}\n## create new variable for difference Size to mean(Size)\ntib <-\n  tib |> \n  dplyr::mutate(Size_c = Size - mean(Size))\n\n## fit model\nlambert.A1.1 <- \n  brms::brm(data = tib, \n      family = gaussian(),\n      Height ~ 1 + Size_c,\n      prior = c(brms::prior(normal(70, 8), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, lb = 0),\n                brms::prior(uniform(0, 20), class = sigma, ub = 20)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 42,\n      file = \"fits/lambert.A1.1\")\n\n## display summary\nbrms:::summary.brmsfit(lambert.A1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: Height ~ 1 + Size_c \n#>    Data: tib (Number of observations: 408) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    68.42      0.10    68.22    68.62 1.00     4314     2960\n#> Size_c        1.77      0.05     1.67     1.88 1.00     3420     2604\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     2.07      0.07     1.93     2.22 1.00     3870     2970\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code #lst-fig-lm-rethinking-priors lst-cap=\"Fit model with priors from the rethinking book, display summary, trace plots and linear correlation\"}\n## show trace plots\nplot(lambert.A1.1, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![Height in inches (vertical) plotted against shoe size in inches (horizontal), with the line at the posterior mean'](90-example-1_files/figure-html/fig-lm-rethinking-priors-1.png){#fig-lm-rethinking-priors width=672}\n:::\n:::\n\n\n\n#### Fit model with priors from `brms::get_prior()` function\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-lm-student_t-prior lst-cap=\"Fit model with priors from the get_prior function, display summary, trace plots and linear correlation\"}\nmy_priors <- brms::get_prior(Height ~ 1 + Size_c, data = tib, family = gaussian())\n\n## create new variable for difference Size to mean(Size)\ntib <-\n  tib |> \n  dplyr::mutate(Size_c = Size - mean(Size))\n\n## fit model\nlambert.A1.2 <- \n  brms::brm(data = tib, \n      family = gaussian(),\n      Height ~ 1 + Size_c,\n      prior = c(brms::prior(student_t(3, 68, 4.4), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, lb = 1),\n                brms::prior(student_t(3, 0, 4.4), class = sigma, lb = 0)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 42,\n      file = \"fits/lambert.A1.2\")\n\n## display summary\nbrms:::summary.brmsfit(lambert.A1.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: Height ~ 1 + Size_c \n#>    Data: tib (Number of observations: 408) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    68.42      0.10    68.21    68.63 1.00     3532     2798\n#> Size_c        1.78      0.05     1.68     1.87 1.00     4219     2943\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     2.08      0.07     1.93     2.23 1.00     4238     2883\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code #lst-fig-lm-student_t-prior lst-cap=\"Fit model with priors from the get_prior function, display summary, trace plots and linear correlation\"}\n## show trace plots\nplot(lambert.A1.2, widths = c(1, 2))\n\n## plot linear correlation\nlabels <-\n  c(-5, -2.5, 0, 2.5, 5) + mean(tib$Size) |>  \n  round(digits = 0)\n\n\ntib |> \n  ggplot2::ggplot(ggplot2::aes(x = Size_c, y = Height)) +\n  ggplot2::geom_abline(intercept = brms:::fixef.brmsfit(lambert.A1.2)[[1]], \n              slope     = brms:::fixef.brmsfit(lambert.A1.2)[[2]]) +\n  ggplot2::geom_point(position = \"jitter\") +\n  ggplot2::scale_x_continuous(\"Size\",\n                     breaks = c(-5, -2.5, 0, 2.5, 5),\n                     labels = labels) +\n  ggplot2::theme_bw() +\n  ggplot2::theme(panel.grid = ggplot2::element_blank())\n\nmy_priors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                  prior     class   coef group resp dpar nlpar lb ub\n#>                 (flat)         b                                   \n#>                 (flat)         b Size_c                            \n#>  student_t(3, 68, 4.4) Intercept                                   \n#>   student_t(3, 0, 4.4)     sigma                               0   \n#>        source\n#>       default\n#>  (vectorized)\n#>       default\n#>       default\n```\n:::\n\n::: {.cell-output-display}\n![Height in inches (vertical) plotted against shoe size in inches (horizontal), with the line at the posterior mean'](90-example-1_files/figure-html/fig-lm-student_t-prior-1.png){#fig-lm-student_t-prior-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Height in inches (vertical) plotted against shoe size in inches (horizontal), with the line at the posterior mean'](90-example-1_files/figure-html/fig-lm-student_t-prior-2.png){#fig-lm-student_t-prior-2 width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract posterior samples of population-level effects \nsamples1 <- brms::as_draws_df(lambert.A1.1)\nhead(samples1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A draws_df: 6 iterations, 1 chains, and 5 variables\n#>   b_Intercept b_Size_c sigma lprior lp__\n#> 1          68      1.7   2.0   -7.6 -881\n#> 2          68      1.8   2.1   -7.7 -881\n#> 3          69      1.8   2.0   -7.7 -882\n#> 4          68      1.7   2.1   -7.6 -882\n#> 5          69      1.8   2.0   -7.7 -882\n#> 6          69      1.8   2.1   -7.7 -881\n#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n:::\n:::\n\n\n## Glossary {#sec-glossary-A1}\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Contour Plot </td>\n   <td style=\"text-align:left;\"> Contour plots are a way to show a three-dimensional surface on a two-dimensional plane. A contour plot is appropriate if you want to see how some value Z changes as a function of two inputs, X and Y: `z = f(x, y)`. Contour lines indicate levels that are the same. ([Statistics How To](https://www.statisticshowto.com/contour-plots/)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Linear Model </td>\n   <td style=\"text-align:left;\"> A linear model specifies a linear relationship between a dependent variable and $n$ independent variables. It conforms to a mathematical model represented by a linear equation of the form $Y = b_{1}X_{1} + b_{2}X_{2} + … + b_{n}X_{n}$. ([Oxford Reference](https://www.oxfordreference.com/display/10.1093/oi/authority.20110803100107198)) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Prior Predictive Simulation </td>\n   <td style=\"text-align:left;\"> It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Probability Distribution </td>\n   <td style=\"text-align:left;\"> The two defining features are: (1) All values of the distribution must be real and non-negative. (2) The sum (for discrete random variables) or integral (for continuous random variables) across all possible values of the random variable must be 1. (Bayesian Statistics, Chap.3) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [
      "90-example-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}